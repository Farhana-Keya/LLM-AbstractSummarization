URL,DOI,Title,Abstract,Domain,Sub_domainS,publisher,abstract_source,citation,Published_year,contributor,summary_gpt_4,summary_GPT_4_Turbo,summary_gpt_3.5_turbo_1106,summary_llama_2,summary_falcon_7b_instruct,length(word_count),Human_gpt4_readability,Human_gpt4_salient_entity_check,Human_gpt4_Conciseness_check,Human_gpt4_grammer_check,Human_gpt4_extra_word_existance,Human_GPT_4_Turbo_readability,Human_GPT_4_Turbo_salient_entity_check,Human_GPT_4_Turbo_Conciseness_check,Human_GPT_4_Turbo_grammer_check,HumanGPT_4_Turbo_extra_word_existance,Human_gpt_3.5_turbo_1106_readability,Human_gpt_3.5_turbo_1106_salient_entity_check,Human_gpt_3.5_turbo_1106_Conciseness_check,Human_gpt_3.5_turbo_1106_grammer_check,Humangpt_3.5_turbo_1106_extra_word_existance,Human_llama_2_readability,Human_llama_2_salient_entity_check,Human_llama_2_Conciseness_check,Human_llama_2_grammer_check,Humangpt_llama_2_extra_word_existance,length(word_count)_gpt_4,length(word_count)_GPT_4_Turbo,length(word_count)_gpt_3.5_turbo_1106,length(word_count)_llama_2,gpt_4_Turbo_clarity,gpt_4_Turbo_communication,gpt_4_Turbo_simplification,gpt_4_Turbo_readability,gpt_4_Turbo_entity_inclusion,gpt_4_Turbo_accuracy,gpt_4_Turbo_grammar,gpt_4_Turbo_conciseness,gpt_4_clarity,gpt_4_communication,gpt_4_simplification,gpt_4_readability,gpt_4_entity_inclusion,gpt_4_accuracy,gpt_4_grammar,gpt_4_conciseness,gpt_3.5_turbo_1106_clarity,gpt_3.5_turbo_1106_communication,gpt_3.5_turbo_1106_simplification,gpt_3.5_turbo_1106_readability,gpt_3.5_turbo_1106_entity_inclusion,gpt_3.5_turbo_1106_accuracy,gpt_3.5_turbo_1106_grammar,gpt_3.5_turbo_1106_conciseness,llama_2_clarity,llama_2_communication,llama_2_simplification,llama_2_readability,llama_2_entity_inclusion,llama_2_accuracy,llama_2_grammar,llama_2_conciseness,Average_score_gpt_4_using_GPT_4_evaluator,Average_score_gpt_3.5_turbo_1106_using_GPT_4_evaluator,Average_score_llama_2_using_GPT_4_evaluator,summary_gpt-3.5-turbo-1106_zero_shot_prompting
https://arxiv.org/abs/2004.05150,10.48550/arXiv.1409.3215,Sequence to Sequence Learning with Neural Networks,"Deep Neural Networks (DNNs) are powerful models that have achieved excel- lent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the pas- sive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier",computer science,,Neural Information Processing Systems,Semantic Scholar,,,Farhana,"The paper presents an end-to-end sequence learning method using multilayered Long Short-Term Memory (LSTM) for tasks like English to French translation, demonstrating high accuracy, increased BLEU score, and efficient handling of long sentences, as well as introducing a successful strategy of reversing word order in source sentences to improve performance.","The paper introduces a general sequence learning approach using multilayered Long Short-Term Memory (LSTM) networks, achieving state-of-the-art results on an English to French translation task, and additionally finds that reversing the order of source words improves performance by creating short-term dependencies.","The paper presents an end-to-end approach to sequence learning using multilayered Long Short-Term Memory (LSTM) for mapping input sequences to a fixed dimensionality vector, achieving high BLEU scores in English to French translation and demonstrating sensitivity to word order.","The authors present a deep learning approach for sequence learning, specifically for translating English to French, which achieved a BLEU score of 34.8 on the entire test set and learned sensible phrase and sentence representations that are sensitive to word order and relatively invariant to the active and passive voice.",,,4,4,3,4,4,4,4,3,4,4,4,4,3,4,4,4,4,3,4,4,50,41,39,50,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,4,5,4,4,4,3,4,5,5,5,4,4.25,4.125,4.25,"This paper presents an end-to-end approach using multilayered Long Short-Term Memory (LSTM) for sequence learning, achieving a BLEU score of 34.8 on English to French translation, outperforming a phrase-based SMT system and demonstrating sensitivity to word order and relative invariance to active and passive voice."
https://www.semanticscholar.org/reader/081651b38ff7533550a3adfc1c00da333a8fe86c,,How transferable are features in deep neural networks?,"Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.",computer science,,Neural Information Processing Systems,Semantic Scholar,,,Farhana,"The study investigates the transition from general to specific features in deep convolutional neural networks, indicating that transferability of features is hindered by the higher layer neurons' specialization and optimization difficulties related to co-adapting neurons, while also revealing that initializing a network with transferred features can improve generalization even after fine-tuning to a specific dataset.","The study analyzes how the generality and specificity of neurons change across different layers in deep neural networks, showing that transferability of features diminishes with higher layers and greater task disparity but can still enhance performance compared to random features, with the transition from general to task-specific features being affected by both neuron specialization and unexpected optimization challenges.","Deep neural networks initially learn general features similar to Gabor filters and color blobs, but as they progress through the layers, the neurons become more specialized, which negatively affects transferability due to optimization difficulties.","The abstract discusses the phenomenon of deep neural networks learning general features on the first layer, which are later replaced by specific features on lower layers, and how this process affects the network's transferability.",,,4,4,4,5,3,4,4,4,5,2,3,3,3,4,4,4,4,4,5,5,55,58,34,34,3,4,3,3,5,5,5,3,3,4,3,3,5,5,5,3,3,3,2,3,4,3,5,4,4,4,3,4,5,5,5,4,3.875,3.375,4.25,"This paper experimentally quantifies the generality versus specificity of neurons in each layer of a deep convolutional neural network, revealing that transferability of features decreases as the distance between the base task and target task increases, but initializing a network with transferred features from almost any number of layers can boost generalization even after fine-tuning to the target dataset."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering,"In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.",computer science,,Neural Information Processing Systems,Semantic Scholar,6153.0,,Farhana,"This research paper presents a formulation of convolutional neural networks (CNNs) within the context of spectral graph theory, aiming to generalize CNNs from low-dimensional grids to high-dimensional irregular domains, while offering efficient numerical schemes, the same complexity as classical CNNs, and adaptability to any graph structure.","This research paper presents a formulation of convolutional neural networks (CNNs) within the context of spectral graph theory, aiming to generalize CNNs from low-dimensional grids to high-dimensional irregular domains, while offering efficient numerical schemes, the same complexity as classical CNNs, and adaptability to any graph structure.","The paper presents a formulation of convolutional neural networks (CNNs) in the context of spectral graph theory, enabling fast localized convolutional filters on graphs with linear computational and constant learning complexity and demonstrating effectiveness on MNIST and 20NEWS datasets.","This research paper explores the possibility of applying convolutional neural networks to high-dimensional irregular domains, such as social networks or brain connectomes, by leveraging the mathematical framework of spectral graph theory.",,,4,4,4,5,4,4,4,4,5,3,4,4,4,5,4,3,3,3,4,2,46,37,39,31,3,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4.25,4.25,4.25,"This paper introduces a novel formulation of convolutional neural networks (CNNs) in the context of spectral graph theory, enabling the design of fast localized convolutional filters on high-dimensional irregular domains represented by graphs, with experiments demonstrating the system's ability to learn local, stationary, and compositional features on graphs."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Learning both Weights and Connections for Efficient Neural Network," Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.",computer science,,Neural Information Processing Systems,Semantic Scholar,5351.0,,Farhana,"The research paper presents a method to significantly reduce the storage and computation required by neural networks without impacting their accuracy by learning and maintaining only essential connections, reducing the number of parameters in popular models like AlexNet and VGG-16 by up to 13x.",The paper presents a method that significantly reduces the storage and computational requirements of neural networks by an order of magnitude without compromising accuracy through pruning and retraining to optimize network architecture by keeping only essential connections.,"The research paper introduces a method to reduce the computational and memory requirements of neural networks by learning only the important connections, successfully reducing parameters by up to 13x without loss of accuracy.","The research paper describes a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections, which they demonstrate on the ImageNet dataset and with VGG-16.",,,4,4,4,5,3,4,4,4,5,2,4,4,4,5,3,4,4,4,5,2,44,37,33,41,4,5,4,5,5,5,5,4,4,5,4,5,5,5,5,5,4,5,4,5,5,5,5,5,4,5,4,5,4,4,5,5,4.75,4.75,4.5,"This paper presents a method to significantly reduce the storage and computation requirements of neural networks without sacrificing accuracy by pruning redundant connections, achieving a 9x reduction in parameters for AlexNet and a 13x reduction for VGG-16 on the ImageNet dataset."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,10.1145/3038912.3052569,Neural Collaborative Filtering,"In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation --- collaborative filtering --- on the basis of implicit feedback. Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering --- the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items. By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural network-based Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real-world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance.",computer science,,Neural Information Processing Systems,Semantic Scholar,4446.0,,Farhana,"The paper presents a new effective framework, named NCF, for recommender systems that utilizes deep neural networks for the task of collaborative filtering, improving upon traditional matrix factorization methods and offering superior recommendation performance when tested on two real-world datasets.","The research introduces a Neural network-based Collaborative Filtering (NCF) framework for recommender systems using implicit feedback, replacing traditional matrix factorization with a deep learning architecture to improve recommendation performance, and demonstrating its effectiveness through experiments on two real-world datasets.","The research paper presents the NCF framework, which is a neural network-based collaborative filtering approach that outperforms state-of-the-art methods by leveraging deep layers of neural networks to improve recommendation performance.","Researchers have developed a new deep learning-based approach called Neural Network-based Collaborative Filtering (NCF) to improve recommendation systems, which replaces the traditional matrix factorization method with a neural network that can learn an arbitrary function from data, and demonstrates significant improvements in performance compared to state-of-the-art methods.",,,4,4,4,5,5,4,4,4,5,4,4,4,4,5,4,4,4,4,5,3,40,39,30,47,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4.25,4.25,4.25,"This paper introduces a general framework called NCF (Neural network-based Collaborative Filtering) that utilizes neural networks to address the key problem of collaborative filtering in recommendation systems, achieving significant improvements over existing methods by leveraging deeper layers of neural networks to model user-item interactions."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Neural Ordinary Differential Equations,"We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.",computer science,,Neural Information Processing Systems,Semantic Scholar,3183.0,,Farhana,"The research paper introduces a new family of deep neural network models that utilize continuous-depth models with constant memory cost, capability to adapt evaluation strategies for each input, and trade numerical precision for speed, with training supported by backpropagation through any ODE solver without requiring access to its internal operations.","The paper presents a novel family of deep neural networks called continuous-depth models, which use a neural network to parameterize the derivative of the hidden state and utilize a differential equation solver for output computation, offering benefits such as constant memory cost, adaptive evaluation, precision-speed trade-offs, and end-to-end training capabilities through ODE solvers.","The paper introduces continuous-depth neural network models incorporating black-box differential equation solvers, with applications in continuous-depth residual networks, continuous-time latent variable models, and continuous normalizing flows, showcasing properties such as constant memory cost, adaptive evaluation strategy, and scalable backpropagation through ODE solvers for end-to-end training.","A new family of deep neural network models called continuous-depth models is introduced, which uses a neural network to parameterize the derivative of the hidden state and computes the output using a black-box differential equation solver, allowing for adaptive evaluation strategies and constant memory cost.",,,4,3,3,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,5,4,50,53,45,45,3,3,2,3,5,5,5,3,4,4,3,4,5,5,5,3,3,3,2,3,5,5,5,3,3,4,3,3,5,5,5,4,4.125,3.625,4.0,"The paper introduces a new family of deep neural network models that parameterize the derivative of the hidden state using a neural network, allowing for continuous-depth models with constant memory cost, adaptable evaluation strategy, and the ability to trade numerical precision for speed. The approach is demonstrated in continuous-depth residual networks and continuous-time latent variable models, as well as in the construction of continuous normalizing flows for generative modeling. The key contribution lies in enabling end-to-end training of ordinary differential equations (ODEs) within larger models, without access to the internal operations of the ODE solver."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Neural Discrete Representation Learning,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",computer science,,Neural Information Processing Systems,Semantic Scholar,2522.0,,Farhana,"This research paper presents a novel generative model called Vector Quantised-Variational AutoEncoder (VQ-VAE) that learns useful, unsupervised, discrete representations, enabling high-quality generation of images, videos, and speech and improving speaker conversion and phoneme learning by using vector quantisation to avoid the common 'posterior collapse' issue in VAE frameworks.","The paper introduces the Vector Quantised-Variational AutoEncoder (VQ-VAE), a generative model that learns useful discrete representations without supervision by using vector quantisation to avoid posterior collapse and demonstrating high-quality generation of images, videos, and speech, as well as speaker conversion and unsupervised learning of phonemes.","The paper proposes the Vector Quantised-Variational AutoEncoder (VQ-VAE) as a generative model that learns powerful discrete representations without supervision, addressing issues observed in traditional VAEs and demonstrating utility in generating high-quality images, videos, and speech, as well as speaker conversion and unsupervised phoneme learning.","The authors propose a generative model called VQ-VAE that learns discrete latent representations without supervision, which can generate high-quality images, videos, and speech, as well as perform speaker conversion and unsupervised learning of phonemes, by incorporating ideas from vector quantization and learning a prior.",,,4,3,3,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,5,4,48,45,44,44,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4,4,3,4,5,5,5,4,4.25,4.25,4.25,"This paper introduces the Vector Quantised-Variational AutoEncoder (VQ-VAE), a generative model that addresses the challenge of learning useful representations without supervision by using discrete latent representations learned through vector quantisation, which successfully avoids ""posterior collapse"" and enables high-quality generation of images, videos, and speech, as well as speaker conversion and unsupervised learning of phonemes."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Self-Normalizing Neural Networks,"Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ""scaled exponential linear units"" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",computer science,,Neural Information Processing Systems,Semantic Scholar,1846.0,,Farhana,"The research introduces self-normalizing neural networks (SNNs) which employ ""scaled exponential linear units"" (SELUs) as activation functions, giving them the ability to automatically converge towards zero mean and unit variance, enabling the training of deep networks, strong regularization and robust learning, and they perform significantly better than all competing feed-forward neural network methods on a variety of tasks.","The paper introduces self-normalizing neural networks (SNNs) that utilize scaled exponential linear units (SELUs) to maintain neuron activations at zero mean and unit variance, enabling the training of very deep networks, and demonstrating superior performance on various machine learning tasks compared to standard feed-forward networks and other methods.","The paper introduces self-normalizing neural networks (SNNs) with scaled exponential linear units (SELUs) that enable high-level abstract representations, proving their convergence properties and demonstrating superior performance on various tasks compared to standard feed-forward neural networks (FNNs) and other machine learning methods.","Self-normalizing neural networks (SNNs) are introduced to enable high-level abstract representations by utilizing the Banach fixed-point theorem, which guarantees that activations close to zero mean and unit variance will converge towards zero mean and unit variance even under the presence of noise and perturbations, allowing for the training of deep networks with many layers and strong regularization, and providing highly robust learning.",,,4,3,3,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,5,4,58,48,41,62,4,4,3,4,5,5,5,4,3,4,2,3,5,5,5,3,4,4,3,4,5,5,5,4,3,4,2,3,5,5,5,3,3.75,4.25,3.75,"This paper introduces self-normalizing neural networks (SNNs) with scaled exponential linear units (SELUs) as the activation function, which enable high-level abstract representations and address the problem of vanishing and exploding gradients in deep networks. The authors demonstrate the effectiveness of SNNs by outperforming standard feed-forward neural networks and other machine learning methods on various tasks, including UCI machine learning repository tasks, drug discovery benchmarks, and astronomy tasks."
https://www.semanticscholar.org/paper/Convolutional-Neural-Networks-on-Graphs-with-Fast-Defferrard-Bresson/c41eb895616e453dcba1a70c9b942c5063cc656c,,Neural Tangent Kernel: Convergence and Generalization in Neural Networks,"At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function (which maps input vectors to output vectors) follows the so-called kernel gradient associated with a new object, which we call the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.",computer science,,Neural Information Processing Systems,Semantic Scholar,1831.0,,Farhana,"The research paper demonstrates that the behavior of artificial neural networks (ANNs) during training can be described by a kernel (the Neural Tangent Kernel, NTK), which allows for studying ANNs in function space and suggests early stopping as a theoretically motivated approach, thereby bridging a connection between ANNs and kernel methods.","The paper demonstrates that as artificial neural networks (ANNs) are trained via gradient descent, their evolution can be described by a new kernel called the Neural Tangent Kernel (NTK), which remains constant in the infinite-width limit, enabling the training process to be analyzed in function space, and reveals that convergence rates are fastest along the largest kernel principal components, providing a theoretical motivation for early stopping.","The study shows that the evolution of artificial neural networks during training can be described by the Neural Tangent Kernel, establishing its connection to kernel methods and providing insights into generalization and training convergence in function space.","The evolution of an artificial neural network during training can be described by a kernel, known as the Neural Tangent Kernel (NTK), which connects the network to kernel methods and allows for the study of the generalization features of ANNs.",,,4,3,3,4,4,4,4,4,5,4,4,4,4,4,4,4,4,4,5,4,51,66,37,40,3,3,2,3,5,4,5,3,4,4,3,4,5,5,5,4,3,4,3,3,5,5,5,4,4,4,3,4,5,5,5,4,4.25,4.0,4.25,"This paper demonstrates that artificial neural networks (ANNs) can be connected to kernel methods, and introduces the concept of the Neural Tangent Kernel (NTK) to describe the evolution of ANNs during training. The NTK converges to a limiting kernel in the infinite-width limit, allowing for the study of training in function space and providing insights into convergence and early stopping in least-squares regression."
